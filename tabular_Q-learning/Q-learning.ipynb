{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e432ebb",
   "metadata": {},
   "source": [
    "# Q Learning coding\n",
    "We build a maze environment to train tabular Q-learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.makedirs(\"frame\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabfc8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动作符号\n",
    "action_symbols = {\n",
    "    0: '↑',  \n",
    "    1: '↓',  \n",
    "    2: '←',  \n",
    "    3: '→',  \n",
    "}\n",
    "\n",
    "# 迷宫环境设置\n",
    "maze = np.array([\n",
    "    [2, 0, 0, 0],\n",
    "    [0, -1, -1, 0],\n",
    "    [0, -1, -1, 0],\n",
    "    [0, 0, 0, 0],\n",
    "    [0, -1, -1, 1]\n",
    "])\n",
    "rows, cols = maze.shape\n",
    "start, end = (0, 0), (4, 3)\n",
    "\n",
    "# Q-Learning参数\n",
    "alpha = 0.1   # 学习率\n",
    "gamma = 0.9   # 折扣因子\n",
    "epsilon = 0.5 # 探索概率\n",
    "\n",
    "# 初始化Q表\n",
    "Q = np.zeros((rows, cols, 4))  # 4个动作：上、下、左、右\n",
    "\n",
    "# 动作索引\n",
    "actions = {\n",
    "    0: (-1, 0),  # 上\n",
    "    1: (1, 0),   # 下\n",
    "    2: (0, -1),  # 左\n",
    "    3: (0, 1)    # 右\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b784090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy_with_maze(Q, maze_map, shape=(4, 4), show=False, title=None):\n",
    "    policy = np.argmax(Q, axis=-1).reshape(shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xticks(np.arange(shape[1]+1)-0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(shape[0]+1)-0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"black\", linewidth=1)\n",
    "    ax.tick_params(which=\"both\", bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    if title is None:\n",
    "        ax.set_title(\"Maze Policy\")\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    \n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            state = i * shape[1] + j\n",
    "            if maze_map[i, j] == -1:\n",
    "                # 墙体为黄色\n",
    "                ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, color='gold'))\n",
    "            else:\n",
    "                if maze_map[i, j] == 0:\n",
    "                    # 可通行区域为蓝色\n",
    "                    ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, color='skyblue'))\n",
    "                elif maze_map[i, j] == 1:\n",
    "                    ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, color='red'))\n",
    "                else:\n",
    "                    ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, color='green'))\n",
    "\n",
    "                # 写动作符号\n",
    "                action = policy[i, j]\n",
    "                ax.text(j, i, action_symbols[action], ha='center', va='center', fontsize=20, color='black')\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return fig\n",
    "\n",
    "def plot_q_values_with_maze(Q, maze_map, shape=(4, 4)):\n",
    "    max_q = np.max(Q, axis=-1).reshape(shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"Max Q-Value per State with Maze\")\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            if maze_map[i, j] == -1:\n",
    "                color = 'gold'\n",
    "            else:\n",
    "                norm_val = max_q[i, j] / np.max(max_q) if np.max(max_q) > 0 else 0\n",
    "                color = plt.cm.Blues(norm_val)\n",
    "                ax.text(j, i, f\"{max_q[i, j]:.2f}\", ha='center', va='center', color='black')\n",
    "\n",
    "            ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, color=color))\n",
    "\n",
    "\n",
    "    ax.set_xticks(np.arange(shape[1]+1)-0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(shape[0]+1)-0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"black\", linewidth=1)\n",
    "    ax.tick_params(which=\"both\", bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程\n",
    "max_count = 40\n",
    "success_episodes = []\n",
    "\n",
    "for episode in tqdm(range(5000)):\n",
    "    state = start\n",
    "    count = 0\n",
    "    success = False  # 本轮是否成功的标记\n",
    "\n",
    "    while state != end and count < max_count:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action_index = np.random.choice([0, 1, 2, 3])\n",
    "        else:\n",
    "            action_index = np.argmax(Q[state[0], state[1]])\n",
    "\n",
    "        action = actions[action_index]\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "\n",
    "        # 检查边界和障碍\n",
    "        if 0 <= next_state[0] < rows and 0 <= next_state[1] < cols and maze[next_state[0], next_state[1]] != -1:\n",
    "            reward = 0\n",
    "            if next_state == end:\n",
    "                reward = 1\n",
    "                success = True  # 成功到达终点\n",
    "            Q[state[0], state[1], action_index] += alpha * (\n",
    "                reward + gamma * np.max(Q[next_state[0], next_state[1]]) - Q[state[0], state[1], action_index]\n",
    "            )\n",
    "            state = next_state\n",
    "        else:\n",
    "            Q[state[0], state[1], action_index] += alpha * (-1 - Q[state[0], state[1], action_index])\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "        if success:\n",
    "            success_episodes.append(episode)\n",
    "            if len(success_episodes) > 1:\n",
    "                epsilon = 0.1\n",
    "                \n",
    "    if episode<200:\n",
    "        fig = plot_policy_with_maze(Q, maze, maze.shape, show=False, title=f\"Episode: {episode}, Success: {success}\")\n",
    "        fig.savefig(f\"frames/policy_{episode:04d}.png\")\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631cc8e1",
   "metadata": {},
   "source": [
    "## Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74814e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize successful rollouts\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.scatter(success_episodes, [1]*len(success_episodes), marker='|', color='green')\n",
    "plt.yticks([0, 1], [\"Failure\", \"Success\"])\n",
    "plt.xlabel(\"Episode Index\")\n",
    "plt.title(\"Episode of successful arrival, reward=0 and epsilon=0.5\")\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize policy\n",
    "fig = plot_policy_with_maze(Q, maze, maze.shape, show=False, title=None)\n",
    "fig.savefig(f\"images.png\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Q value\n",
    "plot_q_values_with_maze(Q, maze, maze.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
